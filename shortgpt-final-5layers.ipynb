{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:02:23.754031Z",
     "iopub.status.busy": "2025-03-24T22:02:23.753746Z",
     "iopub.status.idle": "2025-03-24T22:02:38.646796Z",
     "shell.execute_reply": "2025-03-24T22:02:38.645870Z",
     "shell.execute_reply.started": "2025-03-24T22:02:23.754001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Collecting lm_eval\n",
      "  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (1.2.1)\n",
      "Collecting evaluate (from lm_eval)\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting jsonlines (from lm_eval)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval) (2.10.2)\n",
      "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (0.14.0)\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (2.13.6)\n",
      "Collecting pytablewriter (from lm_eval)\n",
      "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting rouge-score>=0.0.4 (from lm_eval)\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting sacrebleu>=1.5.0 (from lm_eval)\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (1.2.2)\n",
      "Collecting sqlitedict (from lm_eval)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (2.5.1+cu121)\n",
      "Collecting tqdm-multiprocess (from lm_eval)\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (4.47.0)\n",
      "Collecting zstandard (from lm_eval)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting word2number (from lm_eval)\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (from lm_eval) (10.5.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval) (5.9.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval) (0.4.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval) (3.2.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval) (5.3.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval) (3.5.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->lm_eval) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm_eval) (0.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval) (75.1.0)\n",
      "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval)\n",
      "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval)\n",
      "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval)\n",
      "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval)\n",
      "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval)\n",
      "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval)\n",
      "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval) (5.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->lm_eval) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Downloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
      "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
      "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
      "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: rouge-score, sqlitedict, word2number\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2f50e69f490969e34089d91ac9a768a8357a87c56785fbb16b55483f0475533f\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=6d847ca90606433eeccca4b6404173453f7eb01067b4fccc789b7aca68d61523\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=b864ac9a1b9a6be82d2b4a27ea9f77e8c5bac7b4a23a888010cdf0d2d6ad36f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
      "Successfully built rouge-score sqlitedict word2number\n",
      "Installing collected packages: word2number, sqlitedict, zstandard, tqdm-multiprocess, tcolorpy, portalocker, pathvalidate, mbstrdecoder, jsonlines, typepy, DataProperty, tabledata, pytablewriter, sacrebleu, rouge-score, evaluate, lm_eval\n",
      "Successfully installed DataProperty-1.1.0 evaluate-0.4.3 jsonlines-4.0.0 lm_eval-0.4.8 mbstrdecoder-1.1.4 pathvalidate-3.2.3 portalocker-3.1.1 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets lm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:02:38.647944Z",
     "iopub.status.busy": "2025-03-24T22:02:38.647686Z",
     "iopub.status.idle": "2025-03-24T22:03:02.057964Z",
     "shell.execute_reply": "2025-03-24T22:03:02.057273Z",
     "shell.execute_reply.started": "2025-03-24T22:02:38.647900Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, Trainer, TrainingArguments\n",
    "from collections import OrderedDict\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from lm_eval import evaluator, tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:03:02.058880Z",
     "iopub.status.busy": "2025-03-24T22:03:02.058653Z",
     "iopub.status.idle": "2025-03-24T22:03:02.086596Z",
     "shell.execute_reply": "2025-03-24T22:03:02.085547Z",
     "shell.execute_reply.started": "2025-03-24T22:03:02.058860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedc3752f89c422286c0abb1daa5b8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:03:08.890850Z",
     "iopub.status.busy": "2025-03-24T22:03:08.890549Z",
     "iopub.status.idle": "2025-03-24T22:03:08.895800Z",
     "shell.execute_reply": "2025-03-24T22:03:08.894840Z",
     "shell.execute_reply.started": "2025-03-24T22:03:08.890825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# function to remove the layers\n",
    "def layer_removal(\n",
    "    model: nn.Module,\n",
    "    layers_to_remove: OrderedDict\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic removal implementation\n",
    "    \"\"\"\n",
    "\n",
    "    for layer_name, layer_idx in layers_to_remove.items():\n",
    "        modules = layer_name.split(\".\")\n",
    "        mod = model\n",
    "        for m in modules[:-1]:\n",
    "            mod = getattr(mod, m)\n",
    "        \n",
    "        if layer_idx is None:\n",
    "            delattr(mod, modules[-1])\n",
    "        else:\n",
    "            delattr(mod, modules[-1])[layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:03:09.799543Z",
     "iopub.status.busy": "2025-03-24T22:03:09.799251Z",
     "iopub.status.idle": "2025-03-24T22:03:09.804770Z",
     "shell.execute_reply": "2025-03-24T22:03:09.803821Z",
     "shell.execute_reply.started": "2025-03-24T22:03:09.799519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this function calculates the importance scores based on cosine similarity of input and output\n",
    "def block_influence(\n",
    "    input_hidden_state: torch.Tensor,\n",
    "    output_hidden_state: torch.Tensor,\n",
    "    angular=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    input_hidden_state: B, S, D\n",
    "    output_hidden_state: B, S, D\n",
    "    \"\"\"\n",
    "    _, _, d = input_hidden_state.shape\n",
    "    input_hidden_state = input_hidden_state.reshape(-1, d)\n",
    "    output_hidden_state = output_hidden_state.reshape(-1, d)\n",
    "\n",
    "    norm_input = input_hidden_state.norm(dim=-1, keepdim=True)\n",
    "    norm_output = output_hidden_state.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sim = (input_hidden_state @ output_hidden_state.T) / (norm_input * norm_output)\n",
    "    sim = sim.diagonal().nan_to_num(nan=0.5)\n",
    "\n",
    "    if angular:\n",
    "        return (torch.arccos(sim) / torch.pi)\n",
    "\n",
    "    return 1 - sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:03:10.696370Z",
     "iopub.status.busy": "2025-03-24T22:03:10.696106Z",
     "iopub.status.idle": "2025-03-24T22:03:10.709592Z",
     "shell.execute_reply": "2025-03-24T22:03:10.708789Z",
     "shell.execute_reply.started": "2025-03-24T22:03:10.696349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ShortHFModel():\n",
    "\n",
    "    def __init__(self, model_name: str, layers_path: str, n_prune_layers: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        HuggingFace Model Wrapper\n",
    "\n",
    "        Args:\n",
    "            model_name (str): HuggingFace model name\n",
    "            layers_path (str): String in dot notation demonstrating how to access layers of the model. Ex: \"model.layers\"\n",
    "            (Optional) n_prune_layers (int): Number of layers to prune. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "        # self.model.params = self.model.to_fp16(self.model.params)\n",
    "        self.model.to(\"cuda\")\n",
    "\n",
    "        modules = layers_path.split(\".\")\n",
    "        mod = self.model\n",
    "        for m in modules:\n",
    "            mod = getattr(mod, m)\n",
    "        self.layers = mod\n",
    "\n",
    "        self.n_prune_layers = n_prune_layers\n",
    "        self.importances = [0 for _ in self.layers]  # layer-wise importance scores\n",
    "\n",
    "    def remove_layers(\n",
    "        self,\n",
    "        layers_to_remove: Optional[List[int]] = [],\n",
    "        angular: Optional[bool] = False\n",
    "    ):\n",
    "        if angular:\n",
    "            assert self.importances, \"Need to compute importances with eval_importance()\"\n",
    "            assert self.n_prune_layers, \"Need number of layers to prune, set `n_prune_layers`\"\n",
    "            start_layer = np.argsort(np.array(self.importances[:-self.n_prune_layers+1]))[0]\n",
    "            layers_to_remove = list(range(start_layer, start_layer + self.n_prune_layers))\n",
    "        elif not layers_to_remove and self.n_prune_layers:\n",
    "            assert self.importances, \"Need to compute importances with eval_importance()\"\n",
    "            layers_to_remove = np.argsort(np.array(self.importances))[:self.n_prune_layers].tolist()\n",
    "\n",
    "        # remove layers in reverse to avoid indexing errors\n",
    "        for layer_idx in sorted(layers_to_remove, reverse=True):\n",
    "            try:\n",
    "                del self.layers[layer_idx]\n",
    "            except IndexError:\n",
    "                print(f\"layer {layer_idx} does not exist, function may have already been called\")\n",
    "                return []\n",
    "        \n",
    "        return layers_to_remove\n",
    "    \n",
    "    def compute_bi(self, hiddens: List[torch.Tensor], angular: bool):\n",
    "        n = 1\n",
    "        if angular:\n",
    "            assert self.n_prune_layers is not None, \"Set number of layers to prune to use angular importance\"\n",
    "            n = self.n_prune_layers\n",
    "\n",
    "        for i in range(len(hiddens) - n):\n",
    "            in_hidden = hiddens[i]\n",
    "            out_hidden = hiddens[i+n]\n",
    "            if angular:\n",
    "                # use only last token for angular distance as described in section 3.2\n",
    "                # https://arxiv.org/pdf/2403.17887.pdf\n",
    "                in_hidden = in_hidden[:,-1:]\n",
    "                out_hidden = out_hidden[:,-1:]\n",
    "            \n",
    "            self.importances[i] += block_influence(\n",
    "                in_hidden,\n",
    "                out_hidden,\n",
    "                angular=angular\n",
    "            ).sum().cpu().item()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def eval_importance(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_seq_len: int,\n",
    "        stride: int = 256,\n",
    "        max_gen_len: int = 0,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        angular: Optional[bool] = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes layer-wise importances over input texts.\n",
    "\n",
    "        NOTE: ShortGPT paper performs no generation during importance computation, which suggests a `max_gen_len`= 0.\n",
    "\n",
    "        Args:\n",
    "            prompts (List[str]): List of prompts.\n",
    "            max_seq_len (int): Maximum sequence length for model input, the sliding window size.\n",
    "            (Optional) stride (int): Number of tokens to skip/shift between each window inference.\n",
    "            (Optional) max_gen_len (int): Maximum length of the generated text sequence.\n",
    "            (Optional) temperature (float): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            (Optional) top_p (float): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            (Optional) angular (bool): Whether to ues angular distance. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        prompt_tokens = self.tokenizer(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = prompt_tokens.input_ids\n",
    "        attn_mask = prompt_tokens.attention_mask\n",
    "\n",
    "        max_prompt_len = max(len(t) for t in input_ids)\n",
    "\n",
    "        # authors use a sliding window of size 1024 with a shift of 256\n",
    "        for start in range(0, max_prompt_len, stride):\n",
    "            seq_ids = (attn_mask.sum(dim=-1) > start).nonzero().squeeze()\n",
    "            seq_ids = seq_ids.unsqueeze(0) if seq_ids.dim() == 0 else seq_ids  # ensure 2d\n",
    "            inputs = input_ids[seq_ids, start:start+max_seq_len]\n",
    "            attn = attn_mask[seq_ids, start:start+max_seq_len]\n",
    "\n",
    "            if max_gen_len == 0:\n",
    "                outputs = self.model(\n",
    "                    input_ids=inputs.to(\"cuda\"),\n",
    "                    attention_mask=attn.to(\"cuda\"),\n",
    "                    output_hidden_states=True,\n",
    "                )\n",
    "            else:\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs.to(\"cuda\"),\n",
    "                    attention_mask=attn.to(\"cuda\"),\n",
    "                    max_new_tokens=max_gen_len, \n",
    "                    do_sample=True,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    output_hidden_states=True,\n",
    "                    return_dict_in_generate=True,\n",
    "                )\n",
    "            \n",
    "            self.compute_bi(outputs.hidden_states, angular=angular)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:03:16.186507Z",
     "iopub.status.busy": "2025-03-24T22:03:16.186094Z",
     "iopub.status.idle": "2025-03-24T22:03:23.236100Z",
     "shell.execute_reply": "2025-03-24T22:03:23.235189Z",
     "shell.execute_reply.started": "2025-03-24T22:03:16.186472Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e278264af34622b0a456c6adb8fca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852b6394ceb14a82b0e63c763e255916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pg19.py:   0%|          | 0.00/6.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for pg19 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/pg19.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"pg19\",split=\"validation\",streaming=True).take(10) # here we have used only 10 samples\n",
    "dataloader = DataLoader(\n",
    "    data,\n",
    "    batch_size=1,\n",
    "    #shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:03:30.256773Z",
     "iopub.status.busy": "2025-03-24T22:03:30.256436Z",
     "iopub.status.idle": "2025-03-24T22:03:55.728608Z",
     "shell.execute_reply": "2025-03-24T22:03:55.727643Z",
     "shell.execute_reply.started": "2025-03-24T22:03:30.256744Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7ae8e1cfb6473cb510af898a674243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2eae564a1b4edba80b27445a493458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9cd5cb20374420a25ed8cd5dcc61b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e8771a24c5488589e876dbe116e571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2334187aed843dd95268c2351332757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daad682329f3495c9324590073803ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2813e8102f947fb9db24c7875a0f99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6c092db6e14c0ea71a3e7cb54cc1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718ff7fbc08443ae978bc5a187901ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f200d6cbc5c41a58b786b0adde0455a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 1024\n",
    "short_model = ShortHFModel(\n",
    "    model_name=\"meta-llama/Llama-3.2-3B\",\n",
    "    layers_path=\"model.layers\",\n",
    "    n_prune_layers=5, # number of layers to prunes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:03:55.729862Z",
     "iopub.status.busy": "2025-03-24T22:03:55.729638Z",
     "iopub.status.idle": "2025-03-24T22:03:55.737113Z",
     "shell.execute_reply": "2025-03-24T22:03:55.736260Z",
     "shell.execute_reply.started": "2025-03-24T22:03:55.729843Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_model.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:03:55.738680Z",
     "iopub.status.busy": "2025-03-24T22:03:55.738395Z",
     "iopub.status.idle": "2025-03-24T22:04:00.148233Z",
     "shell.execute_reply": "2025-03-24T22:04:00.147341Z",
     "shell.execute_reply.started": "2025-03-24T22:03:55.738653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 24,\n",
       "  \"num_hidden_layers\": 28,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.47.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_model.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:04:00.149878Z",
     "iopub.status.busy": "2025-03-24T22:04:00.149546Z",
     "iopub.status.idle": "2025-03-24T22:04:02.718900Z",
     "shell.execute_reply": "2025-03-24T22:04:02.718048Z",
     "shell.execute_reply.started": "2025-03-24T22:04:00.149846Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dhaka is the capital city of Bangladesh. It is the largest city in Bangladesh and the seventh largest city in the world. Dhaka is located on the banks of the Buriganga River. Dhaka is the political, economic and cultural hub of the country. Dhaka is the']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample generation\n",
    "gen = short_model.model.generate(\n",
    "    short_model.tokenizer([\"Dhaka is the capital city of\"], return_tensors='pt').input_ids.to(\"cuda\"),\n",
    "    max_new_tokens=50\n",
    ")\n",
    "short_model.tokenizer.batch_decode(gen, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T22:04:02.720194Z",
     "iopub.status.busy": "2025-03-24T22:04:02.719851Z",
     "iopub.status.idle": "2025-03-24T23:28:28.170815Z",
     "shell.execute_reply": "2025-03-24T23:28:28.169895Z",
     "shell.execute_reply.started": "2025-03-24T22:04:02.720161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe6b918a42e4270888c1dfdbb2298ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (429039 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# calculate the importance score using the pg19's 10 samples\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    prompts = batch['text']\n",
    "\n",
    "    short_model.eval_importance(\n",
    "        prompts=prompts,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        stride=256,\n",
    "        max_gen_len=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:28.172131Z",
     "iopub.status.busy": "2025-03-24T23:28:28.171809Z",
     "iopub.status.idle": "2025-03-24T23:28:28.177056Z",
     "shell.execute_reply": "2025-03-24T23:28:28.176320Z",
     "shell.execute_reply.started": "2025-03-24T23:28:28.172107Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3099771.578125,\n",
       " 1329948.076171875,\n",
       " 1063329.34765625,\n",
       " 1067733.48046875,\n",
       " 1131896.111328125,\n",
       " 1093777.150390625,\n",
       " 1044766.2802734375,\n",
       " 1005813.33203125,\n",
       " 852906.37109375,\n",
       " 816430.4790039062,\n",
       " 798295.890625,\n",
       " 895215.63671875,\n",
       " 773229.3061523438,\n",
       " 831006.26171875,\n",
       " 796643.4296875,\n",
       " 654673.4311523438,\n",
       " 502772.23876953125,\n",
       " 425554.48291015625,\n",
       " 399103.13525390625,\n",
       " 414931.64697265625,\n",
       " 307589.484375,\n",
       " 274580.3447265625,\n",
       " 252607.755859375,\n",
       " 269757.646484375,\n",
       " 289469.71728515625,\n",
       " 342819.599609375,\n",
       " 448260.12158203125,\n",
       " 1546466.08984375]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the importance scores\n",
    "short_model.importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:28.178132Z",
     "iopub.status.busy": "2025-03-24T23:28:28.177846Z",
     "iopub.status.idle": "2025-03-24T23:28:28.196188Z",
     "shell.execute_reply": "2025-03-24T23:28:28.195466Z",
     "shell.execute_reply.started": "2025-03-24T23:28:28.178111Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size before pruning: 5.984 GB\n"
     ]
    }
   ],
   "source": [
    "# Model's size before pruning\n",
    "param_size = 0\n",
    "for param in short_model.model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "\n",
    "buffer_size = 0\n",
    "for buffer in short_model.model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "# Total size in bytes to GB\n",
    "size_all_gb = (param_size + buffer_size) / 1024**3\n",
    "print('Model size before pruning: {:.3f} GB'.format(size_all_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:28.198429Z",
     "iopub.status.busy": "2025-03-24T23:28:28.198162Z",
     "iopub.status.idle": "2025-03-24T23:28:28.213884Z",
     "shell.execute_reply": "2025-03-24T23:28:28.213163Z",
     "shell.execute_reply.started": "2025-03-24T23:28:28.198407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 23, 21, 24, 20]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the least important layers\n",
    "short_model.remove_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:28.215209Z",
     "iopub.status.busy": "2025-03-24T23:28:28.214878Z",
     "iopub.status.idle": "2025-03-24T23:28:28.227104Z",
     "shell.execute_reply": "2025-03-24T23:28:28.226310Z",
     "shell.execute_reply.started": "2025-03-24T23:28:28.215178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-22): 23 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaSdpaAttention(\n",
       "      (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "      (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "      (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the layers after pruning\n",
    "short_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:28.228009Z",
     "iopub.status.busy": "2025-03-24T23:28:28.227756Z",
     "iopub.status.idle": "2025-03-24T23:28:28.240588Z",
     "shell.execute_reply": "2025-03-24T23:28:28.239810Z",
     "shell.execute_reply.started": "2025-03-24T23:28:28.227988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reassign layer_idx to attentions for caching\n",
    "for layer_idx, module in enumerate(short_model.layers):\n",
    "    module.self_attn.layer_idx = layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:28.241754Z",
     "iopub.status.busy": "2025-03-24T23:28:28.241492Z",
     "iopub.status.idle": "2025-03-24T23:28:28.254753Z",
     "shell.execute_reply": "2025-03-24T23:28:28.253857Z",
     "shell.execute_reply.started": "2025-03-24T23:28:28.241735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# update the models config file\n",
    "short_model.model.config.num_hidden_layers = len(short_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:28.255694Z",
     "iopub.status.busy": "2025-03-24T23:28:28.255446Z",
     "iopub.status.idle": "2025-03-24T23:28:28.269895Z",
     "shell.execute_reply": "2025-03-24T23:28:28.269160Z",
     "shell.execute_reply.started": "2025-03-24T23:28:28.255676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 24,\n",
       "  \"num_hidden_layers\": 23,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.47.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_model.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:28.270831Z",
     "iopub.status.busy": "2025-03-24T23:28:28.270592Z",
     "iopub.status.idle": "2025-03-24T23:28:28.284510Z",
     "shell.execute_reply": "2025-03-24T23:28:28.283693Z",
     "shell.execute_reply.started": "2025-03-24T23:28:28.270810Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 5.047 GB\n"
     ]
    }
   ],
   "source": [
    "# models size after pruning\n",
    "param_size = 0\n",
    "for param in short_model.model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "\n",
    "buffer_size = 0\n",
    "for buffer in short_model.model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "# Total size in bytes to GB\n",
    "size_all_gb = (param_size + buffer_size) / 1024**3\n",
    "print('Model size: {:.3f} GB'.format(size_all_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:28.285832Z",
     "iopub.status.busy": "2025-03-24T23:28:28.285505Z",
     "iopub.status.idle": "2025-03-24T23:28:29.657242Z",
     "shell.execute_reply": "2025-03-24T23:28:29.656364Z",
     "shell.execute_reply.started": "2025-03-24T23:28:28.285804Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dhaka is the capital city of Bangladesh. It has a metropolitan area known as the Greater or Urban area of Dhaka. It is also referred to as the Dharkshipenawetanoparajuitikhadaparajuitekidaparajuthopar']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample generation\n",
    "gen = short_model.model.generate(\n",
    "    short_model.tokenizer([\"Dhaka is the capital city of\"], return_tensors='pt').input_ids.to(\"cuda\"),\n",
    "    max_new_tokens=50\n",
    ")\n",
    "short_model.tokenizer.batch_decode(gen, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the model to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:29.658243Z",
     "iopub.status.busy": "2025-03-24T23:28:29.658013Z",
     "iopub.status.idle": "2025-03-24T23:28:44.820701Z",
     "shell.execute_reply": "2025-03-24T23:28:44.819424Z",
     "shell.execute_reply.started": "2025-03-24T23:28:29.658224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model saved to ./short-llama-3.2-3B-final-5l\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "new_model_name = 'short-llama-3.2-3B-final-5l'\n",
    "output_dir = './'+new_model_name\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "short_model.model.save_pretrained(output_dir)\n",
    "short_model.tokenizer.save_pretrained(output_dir)\n",
    "#new_config.save_pretrained(output_dir)\n",
    "print(f\"Pruned model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T23:28:44.821886Z",
     "iopub.status.busy": "2025-03-24T23:28:44.821572Z",
     "iopub.status.idle": "2025-03-24T23:29:41.280258Z",
     "shell.execute_reply": "2025-03-24T23:29:41.279299Z",
     "shell.execute_reply.started": "2025-03-24T23:28:44.821856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4974e4fcdd8341239e2ab1e30364791c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1883c624c43e4d8a9786cd567158a532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a7e7dcd2c94e5f945d373476493987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/453M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304941b7694c4f1399ab6886db106b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d575f5feb944ec598b9918cea40904c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ecf8ace0bc475b9582003dcfdd73ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Shahrukh0/short-llama-3.2-3B-final-5l/commit/d61ee4968cd70225a898f3186731c12190a1a957', commit_message='Upload tokenizer', commit_description='', oid='d61ee4968cd70225a898f3186731c12190a1a957', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Shahrukh0/short-llama-3.2-3B-final-5l', endpoint='https://huggingface.co', repo_type='model', repo_id='Shahrukh0/short-llama-3.2-3B-final-5l'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the model to your Hugging Face repository\n",
    "\n",
    "short_model.model.push_to_hub(new_model_name, private=False)\n",
    "short_model.tokenizer.push_to_hub(new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
