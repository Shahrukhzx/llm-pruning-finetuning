{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"!pip install datasets lm_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:02:23.753746Z","iopub.execute_input":"2025-03-24T22:02:23.754031Z","iopub.status.idle":"2025-03-24T22:02:38.646796Z","shell.execute_reply.started":"2025-03-24T22:02:23.754001Z","shell.execute_reply":"2025-03-24T22:02:38.645870Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting lm_eval\n  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (1.2.1)\nCollecting evaluate (from lm_eval)\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting jsonlines (from lm_eval)\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval) (2.10.2)\nRequirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (0.14.0)\nRequirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (2.13.6)\nCollecting pytablewriter (from lm_eval)\n  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\nCollecting rouge-score>=0.0.4 (from lm_eval)\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting sacrebleu>=1.5.0 (from lm_eval)\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (1.2.2)\nCollecting sqlitedict (from lm_eval)\n  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (2.5.1+cu121)\nCollecting tqdm-multiprocess (from lm_eval)\n  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval) (4.47.0)\nCollecting zstandard (from lm_eval)\n  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nCollecting word2number (from lm_eval)\n  Downloading word2number-1.1.zip (9.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (from lm_eval) (10.5.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval) (5.9.5)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval) (0.4.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval) (1.17.0)\nCollecting portalocker (from sacrebleu>=1.5.0->lm_eval)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval) (5.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval) (3.5.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->lm_eval) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm_eval) (0.21.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval) (75.1.0)\nCollecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval)\n  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\nCollecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval)\n  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\nCollecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval)\n  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\nCollecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval)\n  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\nCollecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval)\n  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\nCollecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval)\n  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval) (5.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->lm_eval) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\nDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\nDownloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\nDownloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\nDownloading tabledata-1.3.4-py3-none-any.whl (11 kB)\nDownloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\nDownloading typepy-1.3.4-py3-none-any.whl (31 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: rouge-score, sqlitedict, word2number\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2f50e69f490969e34089d91ac9a768a8357a87c56785fbb16b55483f0475533f\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=6d847ca90606433eeccca4b6404173453f7eb01067b4fccc789b7aca68d61523\n  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=b864ac9a1b9a6be82d2b4a27ea9f77e8c5bac7b4a23a888010cdf0d2d6ad36f7\n  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\nSuccessfully built rouge-score sqlitedict word2number\nInstalling collected packages: word2number, sqlitedict, zstandard, tqdm-multiprocess, tcolorpy, portalocker, pathvalidate, mbstrdecoder, jsonlines, typepy, DataProperty, tabledata, pytablewriter, sacrebleu, rouge-score, evaluate, lm_eval\nSuccessfully installed DataProperty-1.1.0 evaluate-0.4.3 jsonlines-4.0.0 lm_eval-0.4.8 mbstrdecoder-1.1.4 pathvalidate-3.2.3 portalocker-3.1.1 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1 zstandard-0.23.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# importing the required libraries\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, Trainer, TrainingArguments\nfrom collections import OrderedDict\nfrom typing import List, Optional\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nimport torch\nfrom torch.utils.data import DataLoader\nfrom lm_eval import evaluator, tasks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:02:38.647686Z","iopub.execute_input":"2025-03-24T22:02:38.647944Z","iopub.status.idle":"2025-03-24T22:03:02.057964Z","shell.execute_reply.started":"2025-03-24T22:02:38.647900Z","shell.execute_reply":"2025-03-24T22:03:02.057273Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:03:02.058653Z","iopub.execute_input":"2025-03-24T22:03:02.058880Z","iopub.status.idle":"2025-03-24T22:03:02.086596Z","shell.execute_reply.started":"2025-03-24T22:03:02.058860Z","shell.execute_reply":"2025-03-24T22:03:02.085547Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dedc3752f89c422286c0abb1daa5b8d6"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Support functions","metadata":{}},{"cell_type":"code","source":"# function to remove the layers\ndef layer_removal(\n    model: nn.Module,\n    layers_to_remove: OrderedDict\n):\n    \"\"\"\n    Generic removal implementation\n    \"\"\"\n\n    for layer_name, layer_idx in layers_to_remove.items():\n        modules = layer_name.split(\".\")\n        mod = model\n        for m in modules[:-1]:\n            mod = getattr(mod, m)\n        \n        if layer_idx is None:\n            delattr(mod, modules[-1])\n        else:\n            delattr(mod, modules[-1])[layer_idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:33:30.520468Z","iopub.execute_input":"2025-03-24T23:33:30.520806Z","iopub.status.idle":"2025-03-24T23:33:30.525742Z","shell.execute_reply.started":"2025-03-24T23:33:30.520781Z","shell.execute_reply":"2025-03-24T23:33:30.524868Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# this function calculates the importance scores based on cosine similarity of input and output\ndef block_influence(\n    input_hidden_state: torch.Tensor,\n    output_hidden_state: torch.Tensor,\n    angular=False,\n):\n    \"\"\"\n    input_hidden_state: B, S, D\n    output_hidden_state: B, S, D\n    \"\"\"\n    _, _, d = input_hidden_state.shape\n    input_hidden_state = input_hidden_state.reshape(-1, d)\n    output_hidden_state = output_hidden_state.reshape(-1, d)\n\n    norm_input = input_hidden_state.norm(dim=-1, keepdim=True)\n    norm_output = output_hidden_state.norm(dim=-1, keepdim=True)\n\n    sim = (input_hidden_state @ output_hidden_state.T) / (norm_input * norm_output)\n    sim = sim.diagonal().nan_to_num(nan=0.5)\n\n    if angular:\n        return (torch.arccos(sim) / torch.pi)\n\n    return 1 - sim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:33:31.054885Z","iopub.execute_input":"2025-03-24T23:33:31.055222Z","iopub.status.idle":"2025-03-24T23:33:31.060459Z","shell.execute_reply.started":"2025-03-24T23:33:31.055197Z","shell.execute_reply":"2025-03-24T23:33:31.059514Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class ShortHFModel():\n\n    def __init__(self, model_name: str, layers_path: str, n_prune_layers: Optional[int] = None):\n        \"\"\"\n        HuggingFace Model Wrapper\n\n        Args:\n            model_name (str): HuggingFace model name\n            layers_path (str): String in dot notation demonstrating how to access layers of the model. Ex: \"model.layers\"\n            (Optional) n_prune_layers (int): Number of layers to prune. Defaults to None.\n        \"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n        # self.model.params = self.model.to_fp16(self.model.params)\n        self.model.to(\"cuda\")\n\n        modules = layers_path.split(\".\")\n        mod = self.model\n        for m in modules:\n            mod = getattr(mod, m)\n        self.layers = mod\n\n        self.n_prune_layers = n_prune_layers\n        self.importances = [0 for _ in self.layers]  # layer-wise importance scores\n\n    def remove_layers(\n        self,\n        layers_to_remove: Optional[List[int]] = [],\n        angular: Optional[bool] = False\n    ):\n        if angular:\n            assert self.importances, \"Need to compute importances with eval_importance()\"\n            assert self.n_prune_layers, \"Need number of layers to prune, set `n_prune_layers`\"\n            start_layer = np.argsort(np.array(self.importances[:-self.n_prune_layers+1]))[0]\n            layers_to_remove = list(range(start_layer, start_layer + self.n_prune_layers))\n        elif not layers_to_remove and self.n_prune_layers:\n            assert self.importances, \"Need to compute importances with eval_importance()\"\n            layers_to_remove = np.argsort(np.array(self.importances))[:self.n_prune_layers].tolist()\n\n        # remove layers in reverse to avoid indexing errors\n        for layer_idx in sorted(layers_to_remove, reverse=True):\n            try:\n                del self.layers[layer_idx]\n            except IndexError:\n                print(f\"layer {layer_idx} does not exist, function may have already been called\")\n                return []\n        \n        return layers_to_remove\n    \n    def compute_bi(self, hiddens: List[torch.Tensor], angular: bool):\n        n = 1\n        if angular:\n            assert self.n_prune_layers is not None, \"Set number of layers to prune to use angular importance\"\n            n = self.n_prune_layers\n\n        for i in range(len(hiddens) - n):\n            in_hidden = hiddens[i]\n            out_hidden = hiddens[i+n]\n            if angular:\n                # use only last token for angular distance as described in section 3.2\n                # https://arxiv.org/pdf/2403.17887.pdf\n                in_hidden = in_hidden[:,-1:]\n                out_hidden = out_hidden[:,-1:]\n            \n            self.importances[i] += block_influence(\n                in_hidden,\n                out_hidden,\n                angular=angular\n            ).sum().cpu().item()\n\n    @torch.inference_mode()\n    def eval_importance(\n        self,\n        prompts: List[str],\n        max_seq_len: int,\n        stride: int = 256,\n        max_gen_len: int = 0,\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        angular: Optional[bool] = False\n    ):\n        \"\"\"\n        Computes layer-wise importances over input texts.\n\n        NOTE: ShortGPT paper performs no generation during importance computation, which suggests a `max_gen_len`= 0.\n\n        Args:\n            prompts (List[str]): List of prompts.\n            max_seq_len (int): Maximum sequence length for model input, the sliding window size.\n            (Optional) stride (int): Number of tokens to skip/shift between each window inference.\n            (Optional) max_gen_len (int): Maximum length of the generated text sequence.\n            (Optional) temperature (float): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            (Optional) top_p (float): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            (Optional) angular (bool): Whether to ues angular distance. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        prompt_tokens = self.tokenizer(\n            prompts,\n            padding=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        input_ids = prompt_tokens.input_ids\n        attn_mask = prompt_tokens.attention_mask\n\n        max_prompt_len = max(len(t) for t in input_ids)\n\n        # authors use a sliding window of size 1024 with a shift of 256\n        for start in range(0, max_prompt_len, stride):\n            seq_ids = (attn_mask.sum(dim=-1) > start).nonzero().squeeze()\n            seq_ids = seq_ids.unsqueeze(0) if seq_ids.dim() == 0 else seq_ids  # ensure 2d\n            inputs = input_ids[seq_ids, start:start+max_seq_len]\n            attn = attn_mask[seq_ids, start:start+max_seq_len]\n\n            if max_gen_len == 0:\n                outputs = self.model(\n                    input_ids=inputs.to(\"cuda\"),\n                    attention_mask=attn.to(\"cuda\"),\n                    output_hidden_states=True,\n                )\n            else:\n                outputs = self.model.generate(\n                    input_ids=inputs.to(\"cuda\"),\n                    attention_mask=attn.to(\"cuda\"),\n                    max_new_tokens=max_gen_len, \n                    do_sample=True,\n                    temperature=temperature,\n                    top_p=top_p,\n                    output_hidden_states=True,\n                    return_dict_in_generate=True,\n                )\n            \n            self.compute_bi(outputs.hidden_states, angular=angular)\n\n        return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:33:32.149256Z","iopub.execute_input":"2025-03-24T23:33:32.149571Z","iopub.status.idle":"2025-03-24T23:33:32.163770Z","shell.execute_reply.started":"2025-03-24T23:33:32.149542Z","shell.execute_reply":"2025-03-24T23:33:32.162856Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Loading the dataset","metadata":{}},{"cell_type":"code","source":"data = load_dataset(\"pg19\",split=\"validation\",streaming=True).take(10) # here we have used only 10 samples\ndataloader = DataLoader(\n    data,\n    batch_size=1,\n    #shuffle=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:33:38.094938Z","iopub.execute_input":"2025-03-24T23:33:38.095234Z","iopub.status.idle":"2025-03-24T23:33:40.858825Z","shell.execute_reply.started":"2025-03-24T23:33:38.095212Z","shell.execute_reply":"2025-03-24T23:33:40.857874Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Loading the model","metadata":{}},{"cell_type":"code","source":"#import os\n#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n#torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_SEQ_LEN = 1024\nshort_model = ShortHFModel(\n    model_name=\"meta-llama/Llama-3.2-3B\",\n    layers_path=\"model.layers\",\n    n_prune_layers=6, # number of layers to prunes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:33:54.572264Z","iopub.execute_input":"2025-03-24T23:33:54.572566Z","iopub.status.idle":"2025-03-24T23:34:16.649362Z","shell.execute_reply.started":"2025-03-24T23:33:54.572542Z","shell.execute_reply":"2025-03-24T23:34:16.648010Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8555534f5544eec9b9212a095ea9a3e"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"short_model.model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:34:16.651234Z","iopub.execute_input":"2025-03-24T23:34:16.651543Z","iopub.status.idle":"2025-03-24T23:34:16.658658Z","shell.execute_reply.started":"2025-03-24T23:34:16.651522Z","shell.execute_reply":"2025-03-24T23:34:16.657772Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 3072)\n    (layers): ModuleList(\n      (0-27): 28 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"short_model.model.config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:34:16.660470Z","iopub.execute_input":"2025-03-24T23:34:16.660755Z","iopub.status.idle":"2025-03-24T23:34:16.681190Z","shell.execute_reply.started":"2025-03-24T23:34:16.660727Z","shell.execute_reply":"2025-03-24T23:34:16.680295Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"LlamaConfig {\n  \"_attn_implementation_autoset\": true,\n  \"_name_or_path\": \"meta-llama/Llama-3.2-3B\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128001,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# sample generation\ngen = short_model.model.generate(\n    short_model.tokenizer([\"Dhaka is the capital city of\"], return_tensors='pt').input_ids.to(\"cuda\"),\n    max_new_tokens=50\n)\nshort_model.tokenizer.batch_decode(gen, skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:34:16.682066Z","iopub.execute_input":"2025-03-24T23:34:16.682318Z","iopub.status.idle":"2025-03-24T23:34:18.339192Z","shell.execute_reply.started":"2025-03-24T23:34:16.682298Z","shell.execute_reply":"2025-03-24T23:34:18.338299Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['Dhaka is the capital city of Bangladesh and one of the most important cities of the country. It is also the largest city of Bangladesh and the main business, cultural, political and educational center of the country. The city is situated in the northern part of the country and is the center']"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"# Pruning","metadata":{}},{"cell_type":"code","source":"# calculate the importance score using the pg19's 10 samples\nfor i, batch in enumerate(tqdm(dataloader)):\n    prompts = batch['text']\n\n    short_model.eval_importance(\n        prompts=prompts,\n        max_seq_len=MAX_SEQ_LEN,\n        stride=256,\n        max_gen_len=0\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:34:18.339943Z","iopub.execute_input":"2025-03-24T23:34:18.340192Z","iopub.status.idle":"2025-03-25T00:58:44.786513Z","shell.execute_reply.started":"2025-03-24T23:34:18.340173Z","shell.execute_reply":"2025-03-25T00:58:44.785596Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2689c4d4cd04217a5354c634b112b33"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (429039 > 131072). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# check the importance scores\nshort_model.importances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:44.788358Z","iopub.execute_input":"2025-03-25T00:58:44.788595Z","iopub.status.idle":"2025-03-25T00:58:44.793641Z","shell.execute_reply.started":"2025-03-25T00:58:44.788576Z","shell.execute_reply":"2025-03-25T00:58:44.792970Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[3099771.578125,\n 1329948.076171875,\n 1063329.34765625,\n 1067733.48046875,\n 1131896.111328125,\n 1093777.150390625,\n 1044766.2802734375,\n 1005813.33203125,\n 852906.37109375,\n 816430.4790039062,\n 798295.890625,\n 895215.63671875,\n 773229.3061523438,\n 831006.26171875,\n 796643.4296875,\n 654673.4311523438,\n 502772.23876953125,\n 425554.48291015625,\n 399103.13525390625,\n 414931.64697265625,\n 307589.484375,\n 274580.3447265625,\n 252607.755859375,\n 269757.646484375,\n 289469.71728515625,\n 342819.599609375,\n 448260.12158203125,\n 1546466.08984375]"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Model's size before pruning\nparam_size = 0\nfor param in short_model.model.parameters():\n    param_size += param.nelement() * param.element_size()\n\nbuffer_size = 0\nfor buffer in short_model.model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\n# Total size in bytes to GB\nsize_all_gb = (param_size + buffer_size) / 1024**3\nprint('Model size before pruning: {:.3f} GB'.format(size_all_gb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:44.801113Z","iopub.execute_input":"2025-03-25T00:58:44.801390Z","iopub.status.idle":"2025-03-25T00:58:44.811131Z","shell.execute_reply.started":"2025-03-25T00:58:44.801361Z","shell.execute_reply":"2025-03-25T00:58:44.810356Z"}},"outputs":[{"name":"stdout","text":"Model size before pruning: 5.984 GB\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# remove the least important layers\nshort_model.remove_layers()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:44.822335Z","iopub.execute_input":"2025-03-25T00:58:44.822587Z","iopub.status.idle":"2025-03-25T00:58:44.828134Z","shell.execute_reply.started":"2025-03-25T00:58:44.822566Z","shell.execute_reply":"2025-03-25T00:58:44.827427Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"[22, 23, 21, 24, 20, 25]"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# check the layers after pruning\nshort_model.layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:44.830950Z","iopub.execute_input":"2025-03-25T00:58:44.831232Z","iopub.status.idle":"2025-03-25T00:58:44.844615Z","shell.execute_reply.started":"2025-03-25T00:58:44.831203Z","shell.execute_reply":"2025-03-25T00:58:44.843878Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"ModuleList(\n  (0-21): 22 x LlamaDecoderLayer(\n    (self_attn): LlamaSdpaAttention(\n      (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n      (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n      (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n      (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n      (rotary_emb): LlamaRotaryEmbedding()\n    )\n    (mlp): LlamaMLP(\n      (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n      (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n      (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n      (act_fn): SiLU()\n    )\n    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n  )\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"# reassign layer_idx to attentions for caching\nfor layer_idx, module in enumerate(short_model.layers):\n    module.self_attn.layer_idx = layer_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:44.895032Z","iopub.execute_input":"2025-03-25T00:58:44.895315Z","iopub.status.idle":"2025-03-25T00:58:44.899536Z","shell.execute_reply.started":"2025-03-25T00:58:44.895288Z","shell.execute_reply":"2025-03-25T00:58:44.898696Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# update the models config file\nshort_model.model.config.num_hidden_layers = len(short_model.layers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:44.903659Z","iopub.execute_input":"2025-03-25T00:58:44.903960Z","iopub.status.idle":"2025-03-25T00:58:44.913084Z","shell.execute_reply.started":"2025-03-25T00:58:44.903932Z","shell.execute_reply":"2025-03-25T00:58:44.912328Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"short_model.model.config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:44.913742Z","iopub.execute_input":"2025-03-25T00:58:44.914002Z","iopub.status.idle":"2025-03-25T00:58:44.930235Z","shell.execute_reply.started":"2025-03-25T00:58:44.913977Z","shell.execute_reply":"2025-03-25T00:58:44.929618Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"LlamaConfig {\n  \"_attn_implementation_autoset\": true,\n  \"_name_or_path\": \"meta-llama/Llama-3.2-3B\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128001,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# models size after pruning\nparam_size = 0\nfor param in short_model.model.parameters():\n    param_size += param.nelement() * param.element_size()\n\nbuffer_size = 0\nfor buffer in short_model.model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\n# Total size in bytes to GB\nsize_all_gb = (param_size + buffer_size) / 1024**3\nprint('Model size: {:.3f} GB'.format(size_all_gb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:44.931065Z","iopub.execute_input":"2025-03-25T00:58:44.931357Z","iopub.status.idle":"2025-03-25T00:58:44.947066Z","shell.execute_reply.started":"2025-03-25T00:58:44.931329Z","shell.execute_reply":"2025-03-25T00:58:44.946269Z"}},"outputs":[{"name":"stdout","text":"Model size: 4.859 GB\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# sample generation\ngen = short_model.model.generate(\n    short_model.tokenizer([\"Dhaka is the capital city of\"], return_tensors='pt').input_ids.to(\"cuda\"),\n    max_new_tokens=50\n)\nshort_model.tokenizer.batch_decode(gen, skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:44.947832Z","iopub.execute_input":"2025-03-25T00:58:44.948111Z","iopub.status.idle":"2025-03-25T00:58:46.277180Z","shell.execute_reply.started":"2025-03-25T00:58:44.948079Z","shell.execute_reply":"2025-03-25T00:58:46.276301Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"['Dhaka is the capital city of Bangladesh, and is the capital of the Dh dakhauh District of the same. It is the capital of the country of land, and the capital of the region of the same. The capital of the capital of the same, and the capital']"},"metadata":{}}],"execution_count":41},{"cell_type":"markdown","source":"# Upload the model to HF","metadata":{}},{"cell_type":"code","source":"import os\nnew_model_name = 'short-llama-3.2-3B-final-6l'\noutput_dir = './'+new_model_name\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nshort_model.model.save_pretrained(output_dir)\nshort_model.tokenizer.save_pretrained(output_dir)\n#new_config.save_pretrained(output_dir)\nprint(f\"Pruned model saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:58:46.278059Z","iopub.execute_input":"2025-03-25T00:58:46.278360Z","iopub.status.idle":"2025-03-25T00:59:00.203957Z","shell.execute_reply.started":"2025-03-25T00:58:46.278332Z","shell.execute_reply":"2025-03-25T00:59:00.201008Z"}},"outputs":[{"name":"stdout","text":"Pruned model saved to ./short-llama-3.2-3B-final-6l\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Push the model to your Hugging Face repository\n\nshort_model.model.push_to_hub(new_model_name, private=False)\nshort_model.tokenizer.push_to_hub(new_model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:59:00.206288Z","iopub.execute_input":"2025-03-25T00:59:00.207804Z","iopub.status.idle":"2025-03-25T00:59:55.982860Z","shell.execute_reply.started":"2025-03-25T00:59:00.207759Z","shell.execute_reply":"2025-03-25T00:59:55.981901Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69505a7209a64096a92bda9f362f1669"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"073ca956a005494fa8d3c90f51e5361e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/252M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c6e0c0a295d4dcea2f11ff25f5c1db8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"602c4957e3694471842fcc5ef822855b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8391f60f75524ec5b96ead1a2a733f4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0bd3dcb30740d3bfb8d78e97a42af4"}},"metadata":{}},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Shahrukh0/short-llama-3.2-3B-final-6l/commit/1c41c9e0ca5e65d0405900d9a2cc8dcc940a7771', commit_message='Upload tokenizer', commit_description='', oid='1c41c9e0ca5e65d0405900d9a2cc8dcc940a7771', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Shahrukh0/short-llama-3.2-3B-final-6l', endpoint='https://huggingface.co', repo_type='model', repo_id='Shahrukh0/short-llama-3.2-3B-final-6l'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_hf_model(model_name, tasks, num_fewshot=0):\n    \"\"\"\n    It calls the evaluator to evaluate a model available on Hugging Face.\n\n    Args:\n    - model_name: The model name in hugging Face.\n    - tasks: Tasks to evaluate.\n    - num_fewshot: Number of examples of few-shot learning\n\n    Returns:\n    - metrics.\n    \"\"\"\n    model_args = f\"pretrained={model_name},device=cuda\"\n    tasks = tasks\n\n    results = evaluator.simple_evaluate(\n      model=\"hf\",\n      model_args=model_args,\n      tasks=tasks,\n      num_fewshot=0,  # Number of few-shot smaples.\n      limit=None,  # Use all the samples in the Evaluate Dataset.\n      bootstrap_iters=10\n    )\n\n    metrics = results.get('results', {})\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:59:55.983719Z","iopub.execute_input":"2025-03-25T00:59:55.983977Z","iopub.status.idle":"2025-03-25T00:59:55.988550Z","shell.execute_reply.started":"2025-03-25T00:59:55.983957Z","shell.execute_reply":"2025-03-25T00:59:55.987679Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Select tasks to evaluate.\ntasks = [\"hellaswag\", \"mmlu\", \"boolq\",'lambada','arc_easy']  # Choose relevant tasks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:59:55.989251Z","iopub.execute_input":"2025-03-25T00:59:55.989477Z","iopub.status.idle":"2025-03-25T00:59:56.006446Z","shell.execute_reply.started":"2025-03-25T00:59:55.989458Z","shell.execute_reply":"2025-03-25T00:59:56.005556Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"metrics_pruned_7l = evaluate_hf_model(\"Shahrukh0/short-llama-3.2-3B-final-7l\", tasks=tasks)\nmetrics_pruned_7l","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from lm_eval.tasks import TaskManager\n\n#print(TaskManager().task_index.keys())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics_base= evaluate_hf_model(\"meta-llama/Llama-3.2-3B\", tasks=tasks)\nmetrics_base","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_metrics(metrics_base, metrics_pruned):\n    \"\"\"\n    Visualizes accuracy and perplexity before and after pruning.\n\n    Parameters:\n    - metrics_base: Dictionary of metrics for the base model\n    - metrics_pruned: Dictionary of metrics for the pruned model\n    \"\"\"\n    datasets = list(metrics_base.keys())\n\n    # Extract accuracy values\n    acc_base = [metrics_base[d].get('acc,none', None) for d in datasets]\n    acc_pruned = [metrics_pruned[d].get('acc,none', None) for d in datasets]\n\n    # Extract perplexity values (ignoring None values)\n    datasets_ppl = [d for d in datasets if 'perplexity,none' in metrics_base[d]]\n    ppl_base = [metrics_base[d]['perplexity,none'] for d in datasets_ppl]\n    ppl_pruned = [metrics_pruned[d]['perplexity,none'] for d in datasets_ppl]\n\n    # Bar Width\n    bar_width = 0.35\n    x = np.arange(len(datasets))\n\n    # Plot Accuracy Comparison\n    plt.figure(figsize=(8, 5))\n    plt.bar(x - bar_width/2, acc_base, bar_width, label=\"Base Model\", color='blue', alpha=0.7)\n    plt.bar(x + bar_width/2, acc_pruned, bar_width, label=\"Pruned Model\", color='red', alpha=0.7)\n    plt.xticks(x, datasets, rotation=20)\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Accuracy Comparison (Base vs. Pruned)\")\n    plt.legend()\n    plt.show()\n\n    # Plot Perplexity Comparison (Only for relevant datasets)\n    if datasets_ppl:\n        x_ppl = np.arange(len(datasets_ppl))\n        plt.figure(figsize=(8, 5))\n        plt.bar(x_ppl - bar_width/2, ppl_base, bar_width, label=\"Base Model\", color='green', alpha=0.7)\n        plt.bar(x_ppl + bar_width/2, ppl_pruned, bar_width, label=\"Pruned Model\", color='orange', alpha=0.7)\n        plt.xticks(x_ppl, datasets_ppl)\n        plt.ylabel(\"Perplexity\")\n        plt.title(\"Perplexity Comparison (Base vs. Pruned)\")\n        plt.legend()\n        plt.show()\n\n# Call the function\nvisualize_metrics(metrics_base, metrics_pruned_7l)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call visualization function\nvisualize_metrics(metrics_base, metrics_pruned_12l, metrics_pruned_4l)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}